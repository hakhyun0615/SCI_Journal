{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bccdb873",
   "metadata": {},
   "source": [
    "## 1. 데이터 로드 및 준비\n",
    "\n",
    "각 모델별로 저장된 예측값과 실제값을 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0654762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_results(model_name, data_path='../데이터/Results/'):\n",
    "    \"\"\"\n",
    "    모델별 예측값과 실제값을 로드하는 함수\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): 모델 이름\n",
    "        data_path (str): 데이터가 저장된 경로\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (predictions, targets) - 예측값과 실제값 리스트\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # pickle 파일로 저장된 경우\n",
    "        with open(f'{data_path}{model_name}_predictions.pkl', 'rb') as f:\n",
    "            predictions = pickle.load(f)\n",
    "        with open(f'{data_path}{model_name}_targets.pkl', 'rb') as f:\n",
    "            targets = pickle.load(f)\n",
    "    except:\n",
    "        # numpy 파일로 저장된 경우\n",
    "        try:\n",
    "            predictions = np.load(f'{data_path}{model_name}_predictions.npy')\n",
    "            targets = np.load(f'{data_path}{model_name}_targets.npy')\n",
    "        except:\n",
    "            print(f\"Warning: {model_name} 데이터를 찾을 수 없습니다. 가상 데이터를 생성합니다.\")\n",
    "            # 가상 데이터 생성 (실제 사용시에는 이 부분을 제거하세요)\n",
    "            np.random.seed(42)\n",
    "            n_samples = 1000\n",
    "            targets = np.random.normal(100, 20, n_samples)\n",
    "            # 모델별로 다른 성능을 시뮬레이션\n",
    "            noise_levels = {\n",
    "                'lstm': 15, 'gru': 14, 'transformer': 12, 'informer': 13,\n",
    "                'pyraformer': 11, 'n_beats': 16, 'nlinear': 18,\n",
    "                'lstm_attention': 10, 'gru_attention': 9, 'transformer_attention': 8,\n",
    "                'informer_attention': 9, 'pyraformer_attention': 7,\n",
    "                'lightgbm': 14, 'catboost': 13\n",
    "            }\n",
    "            noise = noise_levels.get(model_name, 15)\n",
    "            predictions = targets + np.random.normal(0, noise, n_samples)\n",
    "    \n",
    "    return np.array(predictions), np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5c06be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분석할 모델들 정의\n",
    "models = [\n",
    "    'lstm', 'gru', 'transformer', 'informer', 'pyraformer', 'n_beats', 'nlinear',\n",
    "    'lstm_attention', 'gru_attention', 'transformer_attention', 'informer_attention', 'pyraformer_attention',\n",
    "    'lightgbm', 'catboost'\n",
    "]\n",
    "\n",
    "# 각 모델별 데이터 로드\n",
    "model_data = {}\n",
    "model_errors = {}\n",
    "\n",
    "for model in models:\n",
    "    predictions, targets = load_model_results(model)\n",
    "    model_data[model] = {'predictions': predictions, 'targets': targets}\n",
    "    # 절대 오차 계산 (Wilcoxon 검정용)\n",
    "    model_errors[model] = np.abs(predictions - targets)\n",
    "    \n",
    "    print(f\"{model}: 샘플 수 = {len(predictions)}, MAE = {np.mean(model_errors[model]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55116aa",
   "metadata": {},
   "source": [
    "## 2. Wilcoxon 부호-순위 검정 (Wilcoxon Signed-Rank Test)\n",
    "\n",
    "두 모델 간의 성능 차이를 검정합니다. 같은 테스트 데이터에 대한 오차를 비교하여 통계적으로 유의한 차이가 있는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e2d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wilcoxon_pairwise_test(model_errors, alpha=0.05):\n",
    "    \"\"\"\n",
    "    모든 모델 쌍에 대해 Wilcoxon 부호-순위 검정을 수행\n",
    "    \n",
    "    Args:\n",
    "        model_errors (dict): 모델별 오차 딕셔너리\n",
    "        alpha (float): 유의수준\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: 검정 결과를 담은 데이터프레임\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    model_names = list(model_errors.keys())\n",
    "    \n",
    "    for i, model1 in enumerate(model_names):\n",
    "        for j, model2 in enumerate(model_names):\n",
    "            if i < j:  # 중복 방지\n",
    "                # Wilcoxon 부호-순위 검정\n",
    "                statistic, p_value = wilcoxon(model_errors[model1], model_errors[model2])\n",
    "                \n",
    "                # 효과 크기 계산 (r = Z / sqrt(N))\n",
    "                n = len(model_errors[model1])\n",
    "                z_score = statistic / np.sqrt(n * (n + 1) / 6)\n",
    "                effect_size = abs(z_score) / np.sqrt(n)\n",
    "                \n",
    "                # 결과 저장\n",
    "                results.append({\n",
    "                    'Model 1': model1,\n",
    "                    'Model 2': model2,\n",
    "                    'Statistic': statistic,\n",
    "                    'P-value': p_value,\n",
    "                    'Significant': 'Yes' if p_value < alpha else 'No',\n",
    "                    'Effect Size': effect_size,\n",
    "                    'Mean Error Diff': np.mean(model_errors[model1]) - np.mean(model_errors[model2])\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Wilcoxon 검정 실행\n",
    "wilcoxon_results = wilcoxon_pairwise_test(model_errors)\n",
    "\n",
    "# 결과 정렬 (p-value 기준)\n",
    "wilcoxon_results = wilcoxon_results.sort_values('P-value')\n",
    "\n",
    "print(\"=== Wilcoxon 부호-순위 검정 결과 ===\")\n",
    "print(f\"총 {len(wilcoxon_results)} 개의 모델 쌍 비교\")\n",
    "print(f\"유의한 차이가 있는 쌍: {len(wilcoxon_results[wilcoxon_results['Significant'] == 'Yes'])} 개\")\n",
    "print(\"\\n상위 10개 결과:\")\n",
    "print(wilcoxon_results.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7776edaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wilcoxon 검정 결과 시각화\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# P-value 히스토그램\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.hist(wilcoxon_results['P-value'], bins=20, alpha=0.7, edgecolor='black')\n",
    "plt.axvline(x=0.05, color='red', linestyle='--', label='α = 0.05')\n",
    "plt.xlabel('P-value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('P-value Distribution (Wilcoxon Test)')\n",
    "plt.legend()\n",
    "\n",
    "# 효과 크기 분포\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.hist(wilcoxon_results['Effect Size'], bins=20, alpha=0.7, color='green', edgecolor='black')\n",
    "plt.xlabel('Effect Size')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Effect Size Distribution')\n",
    "\n",
    "# 유의한 결과들의 효과 크기\n",
    "significant_results = wilcoxon_results[wilcoxon_results['Significant'] == 'Yes']\n",
    "plt.subplot(2, 2, 3)\n",
    "if len(significant_results) > 0:\n",
    "    plt.scatter(significant_results['P-value'], significant_results['Effect Size'], alpha=0.6)\n",
    "    plt.xlabel('P-value')\n",
    "    plt.ylabel('Effect Size')\n",
    "    plt.title('Significant Results: P-value vs Effect Size')\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'No significant results', ha='center', va='center')\n",
    "    plt.title('No Significant Results Found')\n",
    "\n",
    "# 평균 오차 차이 분포\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.hist(wilcoxon_results['Mean Error Diff'], bins=20, alpha=0.7, color='orange', edgecolor='black')\n",
    "plt.axvline(x=0, color='red', linestyle='-', alpha=0.5)\n",
    "plt.xlabel('Mean Error Difference')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Mean Error Difference Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424f882f",
   "metadata": {},
   "source": [
    "## 3. Friedman 검정 (Friedman Test)\n",
    "\n",
    "여러 모델들 간의 전체적인 성능 차이를 검정합니다. 비모수 ANOVA의 대안으로, 모든 모델들이 동일한 성능을 가진다는 귀무가설을 검정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0649ff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def friedman_test(model_errors):\n",
    "    \"\"\"\n",
    "    Friedman 검정을 수행하고 결과를 출력\n",
    "    \n",
    "    Args:\n",
    "        model_errors (dict): 모델별 오차 딕셔너리\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (statistic, p_value, 검정 결과 딕셔너리)\n",
    "    \"\"\"\n",
    "    # 모든 모델의 오차를 리스트로 변환\n",
    "    error_lists = [model_errors[model] for model in models]\n",
    "    \n",
    "    # Friedman 검정 수행\n",
    "    statistic, p_value = friedmanchisquare(*error_lists)\n",
    "    \n",
    "    # 검정 결과\n",
    "    result = {\n",
    "        'statistic': statistic,\n",
    "        'p_value': p_value,\n",
    "        'significant': p_value < 0.05,\n",
    "        'df': len(models) - 1,\n",
    "        'n_models': len(models),\n",
    "        'n_samples': len(error_lists[0])\n",
    "    }\n",
    "    \n",
    "    return statistic, p_value, result\n",
    "\n",
    "# Friedman 검정 실행\n",
    "friedman_stat, friedman_p, friedman_result = friedman_test(model_errors)\n",
    "\n",
    "print(\"=== Friedman 검정 결과 ===\")\n",
    "print(f\"검정 통계량: {friedman_stat:.4f}\")\n",
    "print(f\"P-value: {friedman_p:.6f}\")\n",
    "print(f\"자유도: {friedman_result['df']}\")\n",
    "print(f\"모델 수: {friedman_result['n_models']}\")\n",
    "print(f\"샘플 수: {friedman_result['n_samples']}\")\n",
    "print(f\"유의성 (α=0.05): {'예' if friedman_result['significant'] else '아니오'}\")\n",
    "\n",
    "if friedman_result['significant']:\n",
    "    print(\"\\n✅ 모델들 간에 통계적으로 유의한 차이가 있습니다.\")\n",
    "    print(\"→ Nemenyi 사후 검정을 진행합니다.\")\n",
    "else:\n",
    "    print(\"\\n❌ 모델들 간에 통계적으로 유의한 차이가 없습니다.\")\n",
    "    print(\"→ 사후 검정이 필요하지 않습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb50b963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델별 순위 계산 (낮은 오차가 더 좋은 순위)\n",
    "def calculate_rankings(model_errors):\n",
    "    \"\"\"각 샘플에 대해 모델들의 순위를 계산\"\"\"\n",
    "    n_samples = len(list(model_errors.values())[0])\n",
    "    rankings = {model: [] for model in models}\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # i번째 샘플에서 각 모델의 오차\n",
    "        sample_errors = [(model, model_errors[model][i]) for model in models]\n",
    "        # 오차 기준으로 정렬 (낮은 오차가 좋은 순위)\n",
    "        sample_errors.sort(key=lambda x: x[1])\n",
    "        \n",
    "        # 순위 할당\n",
    "        for rank, (model, _) in enumerate(sample_errors, 1):\n",
    "            rankings[model].append(rank)\n",
    "    \n",
    "    return rankings\n",
    "\n",
    "# 순위 계산\n",
    "rankings = calculate_rankings(model_errors)\n",
    "\n",
    "# 평균 순위 계산\n",
    "mean_rankings = {model: np.mean(rankings[model]) for model in models}\n",
    "sorted_rankings = sorted(mean_rankings.items(), key=lambda x: x[1])\n",
    "\n",
    "print(\"\\n=== 모델별 평균 순위 (낮을수록 좋음) ===\")\n",
    "for i, (model, rank) in enumerate(sorted_rankings, 1):\n",
    "    print(f\"{i:2d}. {model:20s}: {rank:.3f}\")\n",
    "\n",
    "# 평균 순위 시각화\n",
    "plt.figure(figsize=(12, 6))\n",
    "models_sorted, ranks_sorted = zip(*sorted_rankings)\n",
    "colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(models_sorted)))\n",
    "\n",
    "bars = plt.bar(range(len(models_sorted)), ranks_sorted, color=colors, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Average Rank')\n",
    "plt.title('Average Ranking of Models (Lower is Better)')\n",
    "plt.xticks(range(len(models_sorted)), models_sorted, rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 각 막대 위에 순위 값 표시\n",
    "for i, (bar, rank) in enumerate(zip(bars, ranks_sorted)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "             f'{rank:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab25395",
   "metadata": {},
   "source": [
    "## 4. Nemenyi 사후 검정 (Nemenyi Post-hoc Test)\n",
    "\n",
    "Friedman 검정에서 유의한 차이가 발견된 경우, 어떤 모델들 간에 구체적으로 유의한 차이가 있는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b740e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if friedman_result['significant']:\n",
    "    # 데이터를 Nemenyi 검정에 맞는 형태로 변환\n",
    "    # 각 행은 하나의 관측치(샘플), 각 열은 하나의 그룹(모델)\n",
    "    n_samples = len(list(model_errors.values())[0])\n",
    "    data_matrix = np.zeros((n_samples, len(models)))\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        data_matrix[:, i] = model_errors[model]\n",
    "    \n",
    "    # DataFrame으로 변환\n",
    "    df_nemenyi = pd.DataFrame(data_matrix, columns=models)\n",
    "    \n",
    "    print(\"=== Nemenyi 사후 검정 실행 중... ===\")\n",
    "    \n",
    "    try:\n",
    "        # Nemenyi 검정 수행\n",
    "        nemenyi_results = sp.posthoc_nemenyi_friedman(df_nemenyi)\n",
    "        \n",
    "        print(\"\\n=== Nemenyi 사후 검정 결과 ===\")\n",
    "        print(\"P-value 행렬 (α=0.05 기준):\")\n",
    "        print(nemenyi_results.round(4))\n",
    "        \n",
    "        # 유의한 차이가 있는 모델 쌍 찾기\n",
    "        significant_pairs = []\n",
    "        for i in range(len(models)):\n",
    "            for j in range(i+1, len(models)):\n",
    "                p_val = nemenyi_results.iloc[i, j]\n",
    "                if p_val < 0.05:\n",
    "                    model1, model2 = models[i], models[j]\n",
    "                    rank1, rank2 = mean_rankings[model1], mean_rankings[model2]\n",
    "                    significant_pairs.append({\n",
    "                        'Model 1': model1,\n",
    "                        'Model 2': model2,\n",
    "                        'P-value': p_val,\n",
    "                        'Rank 1': rank1,\n",
    "                        'Rank 2': rank2,\n",
    "                        'Rank Diff': abs(rank1 - rank2)\n",
    "                    })\n",
    "        \n",
    "        if significant_pairs:\n",
    "            print(f\"\\n✅ 총 {len(significant_pairs)}개의 모델 쌍에서 유의한 차이 발견:\")\n",
    "            sig_df = pd.DataFrame(significant_pairs)\n",
    "            sig_df = sig_df.sort_values('P-value')\n",
    "            print(sig_df.to_string(index=False))\n",
    "        else:\n",
    "            print(\"\\n❌ 사후 검정에서 유의한 차이를 보이는 모델 쌍이 없습니다.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Nemenyi 검정 중 오류 발생: {e}\")\n",
    "        print(\"대안으로 Bonferroni 보정된 Wilcoxon 검정을 수행합니다.\")\n",
    "        \n",
    "        # Bonferroni 보정\n",
    "        n_comparisons = len(wilcoxon_results)\n",
    "        bonferroni_alpha = 0.05 / n_comparisons\n",
    "        \n",
    "        bonferroni_significant = wilcoxon_results[wilcoxon_results['P-value'] < bonferroni_alpha]\n",
    "        \n",
    "        print(f\"\\nBonferroni 보정 결과 (α = {bonferroni_alpha:.6f}):\")\n",
    "        if len(bonferroni_significant) > 0:\n",
    "            print(f\"유의한 차이가 있는 쌍: {len(bonferroni_significant)}개\")\n",
    "            print(bonferroni_significant[['Model 1', 'Model 2', 'P-value', 'Mean Error Diff']].to_string(index=False))\n",
    "        else:\n",
    "            print(\"Bonferroni 보정 후에도 유의한 차이를 보이는 쌍이 없습니다.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nFriedman 검정에서 유의한 차이가 없으므로 사후 검정을 수행하지 않습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f06b437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nemenyi 검정 결과 히트맵 시각화 (검정이 성공한 경우)\n",
    "if friedman_result['significant']:\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # P-value 히트맵\n",
    "        mask = np.triu(np.ones_like(nemenyi_results, dtype=bool))\n",
    "        sns.heatmap(nemenyi_results, mask=mask, annot=True, cmap='RdYlBu_r', \n",
    "                   center=0.05, square=True, fmt='.3f', cbar_kws={\"shrink\": .8})\n",
    "        plt.title('Nemenyi Post-hoc Test Results\\n(P-values, α=0.05)')\n",
    "        plt.ylabel('Models')\n",
    "        plt.xlabel('Models')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 유의성 히트맵 (유의한 차이가 있으면 1, 없으면 0)\n",
    "        significance_matrix = (nemenyi_results < 0.05).astype(int)\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(significance_matrix, mask=mask, annot=True, cmap='RdBu_r',\n",
    "                   center=0.5, square=True, fmt='d', cbar_kws={\"shrink\": .8})\n",
    "        plt.title('Statistical Significance Matrix\\n(1: Significant difference, 0: No significant difference)')\n",
    "        plt.ylabel('Models')\n",
    "        plt.xlabel('Models')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except:\n",
    "        print(\"히트맵 생성 중 오류가 발생했습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38b9e69",
   "metadata": {},
   "source": [
    "## 5. 결과 요약 및 결론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e555367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"                     최종 결과 요약\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. 기본 통계\n",
    "print(\"\\n1. 기본 통계:\")\n",
    "print(f\"   - 분석 모델 수: {len(models)}개\")\n",
    "print(f\"   - 테스트 샘플 수: {len(list(model_errors.values())[0])}개\")\n",
    "print(f\"   - 모델 쌍 비교 수: {len(wilcoxon_results)}개\")\n",
    "\n",
    "# 2. Wilcoxon 검정 결과\n",
    "n_significant_wilcoxon = len(wilcoxon_results[wilcoxon_results['Significant'] == 'Yes'])\n",
    "print(f\"\\n2. Wilcoxon 부호-순위 검정:\")\n",
    "print(f\"   - 유의한 차이가 있는 모델 쌍: {n_significant_wilcoxon}개 / {len(wilcoxon_results)}개\")\n",
    "print(f\"   - 유의성 비율: {n_significant_wilcoxon/len(wilcoxon_results)*100:.1f}%\")\n",
    "\n",
    "# 3. Friedman 검정 결과\n",
    "print(f\"\\n3. Friedman 검정:\")\n",
    "print(f\"   - 검정 통계량: {friedman_stat:.4f}\")\n",
    "print(f\"   - P-value: {friedman_p:.6f}\")\n",
    "print(f\"   - 전체적인 모델 간 차이: {'있음' if friedman_result['significant'] else '없음'}\")\n",
    "\n",
    "# 4. 모델 순위\n",
    "print(f\"\\n4. 모델 성능 순위 (평균 순위 기준):\")\n",
    "for i, (model, rank) in enumerate(sorted_rankings[:5], 1):\n",
    "    print(f\"   {i}위: {model} (평균 순위: {rank:.3f})\")\n",
    "if len(sorted_rankings) > 5:\n",
    "    print(f\"   ... (총 {len(sorted_rankings)}개 모델)\")\n",
    "\n",
    "# 5. 사후 검정 결과\n",
    "if friedman_result['significant']:\n",
    "    try:\n",
    "        if 'significant_pairs' in locals() and significant_pairs:\n",
    "            print(f\"\\n5. Nemenyi 사후 검정:\")\n",
    "            print(f\"   - 유의한 차이가 있는 모델 쌍: {len(significant_pairs)}개\")\n",
    "            print(f\"   - 가장 큰 성능 차이: {max([p['Rank Diff'] for p in significant_pairs]):.3f}\")\n",
    "        else:\n",
    "            print(f\"\\n5. 사후 검정 결과: 유의한 차이를 보이는 모델 쌍 없음\")\n",
    "    except:\n",
    "        print(f\"\\n5. 사후 검정: 오류로 인해 완료되지 않음\")\n",
    "else:\n",
    "    print(f\"\\n5. 사후 검정: Friedman 검정에서 유의한 차이가 없어 수행하지 않음\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"분석 완료!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a825b26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy import stats\n",
    "from scipy.stats import friedmanchisquare, wilcoxon\n",
    "import scikit_posthocs as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268835d6",
   "metadata": {},
   "source": [
    "# 통계 검정 분석\n",
    "\n",
    "이 노트북에서는 다양한 모델들의 성능을 비교하기 위해 다음 통계 검정들을 수행합니다:\n",
    "\n",
    "1. **Wilcoxon 부호-순위 검정 (Wilcoxon Signed-Rank Test)**\n",
    "   - 두 모델 간의 성능 차이를 검정\n",
    "   - 쌍체 비모수 검정\n",
    "\n",
    "2. **Friedman 검정 (Friedman Test)**\n",
    "   - 여러 모델들 간의 전체적인 성능 차이를 검정\n",
    "   - 비모수 ANOVA의 대안\n",
    "\n",
    "3. **Nemenyi 사후 검정 (Nemenyi Post-hoc Test)**\n",
    "   - Friedman 검정에서 유의한 차이가 발견된 경우\n",
    "   - 어떤 모델들 간에 유의한 차이가 있는지 확인"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
