{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "import joblib\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "from Dataset.Embedding_Dataset import Embedding_Dataset\n",
    "from Model.Embedding import Embedding\n",
    "\n",
    "from Dataset.Apartment_Complex_Dataset import Apartment_Complex_Dataset\n",
    "from Model.LSTM import LSTM\n",
    "from Model.GRU import GRU\n",
    "from Model.Transformer import Transformer\n",
    "\n",
    "from Dataset.District_Dataset import District_Dataset\n",
    "from Model.LSTM_Attention import LSTMAttention\n",
    "from Model.GRU_Attention import GRUAttention\n",
    "from Model.Transformer_Attention import TransformerAttention\n",
    "\n",
    "from utils import RMSE, rmse, mse, mae, save_train_val_losses\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# connection_info = \"host=localhost dbname=postgres user=postgres password=hd219833 port=5432\"\n",
    "# conn = psycopg2.connect(connection_info)\n",
    "# table_1_query = '''\n",
    "#     SELECT * FROM building\n",
    "#     '''\n",
    "# table_2_query = '''\n",
    "#     SELECT * FROM economy\n",
    "#     '''\n",
    "# table_3_query = '''\n",
    "#     SELECT * FROM building_price\n",
    "#     '''\n",
    "# table_1 = pd.read_sql(table_1_query,conn) \n",
    "# table_2 = pd.read_sql(table_2_query,conn)\n",
    "# table_3 = pd.read_sql(table_3_query,conn) \n",
    "\n",
    "table_1 = pd.read_csv('../데이터/Table/table_1.csv') \n",
    "table_2 = pd.read_csv('../데이터/Table/table_2.csv') \n",
    "table_3 = pd.read_csv('../데이터/Table/table_3.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "lr = 1e-4\n",
    "batch = 64\n",
    "hidden_dim = 1024\n",
    "sub = True\n",
    "embedding_dim = 1024\n",
    "window_size = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (15) must match the existing size (17) at non-singleton dimension 0.  Target sizes: [15, 1].  Tensor sizes: [17, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m embedding_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../데이터/Checkpoint/embedding/default/embedding_lr_0.0001_batch_64_sub_True_emb_1024_ws_12_epochs_13.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mDEVICE)\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m District_Dataset(embedding_model, table_1, table_2, table_3, embedding_dim, window_size, sub, DEVICE)\n\u001b[1;32m      3\u001b[0m dataset_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset)\n\u001b[1;32m      4\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(train_ratio \u001b[38;5;241m*\u001b[39m dataset_length)\n",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36mDistrict_Dataset.__init__\u001b[0;34m(self, model, table_1, table_2, table_3, embedding_dim, window_size, SUB, DEVICE)\u001b[0m\n\u001b[1;32m     89\u001b[0m district_apartment_complexes_prices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(district_apartment_complexes_aids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlen\u001b[39m(table_2_copy), \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(DEVICE)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;66;03m# (동 안의 단지 개수, 204/20, 1)\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, district_apartment_complex_aid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mrange\u001b[39m(district_apartment_complexes_aids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), district_apartment_complexes_aids): \u001b[38;5;66;03m# 동 안의 단지 개수, 동 안의 단지들의 aids\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m     \u001b[43mdistrict_apartment_complexes_prices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdid\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(table_2_copy))})\u001b[38;5;241m.\u001b[39mmerge(table_3_copy[table_3_copy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m district_apartment_complex_aid][[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdid\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m]], on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdid\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdid\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues) \u001b[38;5;66;03m# (204/20, 1)\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embedding_dim \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;66;03m# 임베딩 벡터가 없을 때\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     district_apartment_complexes_embedding_matrixes \u001b[38;5;241m=\u001b[39m encoder_input_tensors\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (15) must match the existing size (17) at non-singleton dimension 0.  Target sizes: [15, 1].  Tensor sizes: [17, 1]"
     ]
    }
   ],
   "source": [
    "embedding_model = torch.load(\"../데이터/Checkpoint/embedding/default/embedding_lr_0.0001_batch_64_sub_True_emb_1024_ws_12_epochs_13.pth\", map_location=DEVICE)\n",
    "dataset = District_Dataset(embedding_model, table_1, table_2, table_3, embedding_dim, window_size, sub, DEVICE)\n",
    "dataset_length = len(dataset)\n",
    "train_size = int(train_ratio * dataset_length)\n",
    "# train_indices = range(0, train_size)\n",
    "val_size = int(val_ratio * dataset_length)\n",
    "# val_indices = range(train_size, train_size + val_size)\n",
    "test_size = int(test_ratio * dataset_length)\n",
    "test_indices = range(train_size + val_size, dataset_length)\n",
    "# train_dataset = Subset(dataset, train_indices)\n",
    "# val_dataset = Subset(dataset, val_indices)\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "# model = torch.load(\"../데이터/Checkpoint/lstm/default/lstm_lr_0.0001_batch_64_hid_1024_sub_True_emb_1024_ws_12_epochs_10.pth\", map_location=DEVICE)\n",
    "\n",
    "# GRU\n",
    "# model = torch.load(\"../데이터/Checkpoint/gru/default/gru_lr_0.0001_batch_64_hid_1024_sub_True_emb_1024_ws_12_epochs_9.pth\", map_location=DEVICE)\n",
    "\n",
    "# transformer\n",
    "# model = torch.load(\"../데이터/Checkpoint/transformer/default/transformer_lr_0.0001_batch_64_sub_True_emb_1024_ws_12_epochs_15.pth\", map_location=DEVICE)\n",
    "\n",
    "# LSTM attention\n",
    "# model = torch.load(\"../데이터/Checkpoint/lstm/attention/lstm_attention_lr_0.0001_batch_64_sub_True_emb_1024_ws_12_epochs_8.pth\", map_location=DEVICE)\n",
    "\n",
    "# GRU attention\n",
    "# model = torch.load(\"../데이터/Checkpoint/gru/attention/gru_attention_lr_0.0001_batch_64_sub_True_emb_1024_ws_12_epochs_4.pth\", map_location=DEVICE)\n",
    "\n",
    "# transformer attention\n",
    "model = torch.load(\"../데이터/Checkpoint/transformer/attention/default/transformer_attention_lr_0.0001_batch_64_sub_True_emb_1024_ws_12_epochs_5.pth\", map_location=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 2.7736\n",
      "Test MSE: 7.6931\n",
      "Test MAE: 2.1420\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_rmses = []\n",
    "test_mses = []\n",
    "test_maes = []\n",
    "\n",
    "test_outputs = []\n",
    "test_trgs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        src = data[0][0].to(DEVICE)\n",
    "        max_len = data[1][0].to(DEVICE)\n",
    "        try:\n",
    "            anw = torch.nonzero(data[2][0]).to(DEVICE)[0]\n",
    "        except:\n",
    "            continue\n",
    "        trg = data[3][0].to(DEVICE)\n",
    "\n",
    "        for index in anw:\n",
    "            # LSTM\n",
    "            # output, _, _ = model(src)\n",
    "            \n",
    "            # GRU\n",
    "            # output, _ = model(src)\n",
    "            \n",
    "            # nlinear\n",
    "            # output, _ = model(src)\n",
    "            \n",
    "            # transformer\n",
    "            # src_mask = model.generate_square_subsequent_mask(src.shape[1]).to(src.device)\n",
    "            # output, _ = model(src, src_mask)\n",
    "            \n",
    "            # test_outputs.append(output[index])\n",
    "            # test_trgs.append(trg[index])\n",
    "\n",
    "            # attention\n",
    "            output = model(src, index, max_len)\n",
    "\n",
    "            test_outputs.append(output)\n",
    "            test_trgs.append(trg[index])\n",
    "\n",
    "save_path = f'../데이터/Checkpoint/transformer/attention/default/transformer_attention_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}_epochs_{5}'\n",
    "with open(f'{save_path}_test_rmses.txt', 'w') as f:\n",
    "    for item in test_rmses:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "with open(f'{save_path}_test_mses.txt', 'w') as f:\n",
    "    for item in test_mses:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "with open(f'{save_path}_test_maes.txt', 'w') as f:\n",
    "    for item in test_maes:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "test_outputs = torch.FloatTensor(test_outputs)\n",
    "test_trgs = torch.FloatTensor(test_trgs)  \n",
    "\n",
    "test_rmse = rmse(test_outputs, test_trgs)\n",
    "test_mse = mse(test_outputs, test_trgs)\n",
    "test_mae = mae(test_outputs, test_trgs)\n",
    "\n",
    "print(f'Test RMSE: {test_rmse:.4f}')\n",
    "print(f'Test MSE: {test_mse:.4f}')\n",
    "print(f'Test MAE: {test_mae:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = torch.load('../데이터/Checkpoint/embedding/default/embedding_lr_0.0001_batch_64_sub_True_emb_1024_ws_12_epochs_13.pth', map_location=DEVICE)\n",
    "dataset = Apartment_Complex_Dataset(embedding_model, table_1, table_2, table_3, embedding_dim, window_size, 'ML', DEVICE)\n",
    "dataset_length = len(dataset)\n",
    "train_size = int(train_ratio * dataset_length)\n",
    "# train_indices = range(0, train_size)\n",
    "val_size = int(val_ratio * dataset_length)\n",
    "# val_indices = range(train_size, train_size + val_size)\n",
    "test_size = int(test_ratio * dataset_length)\n",
    "test_indices = range(train_size + val_size, dataset_length)\n",
    "# train_dataset = Subset(dataset, train_indices)\n",
    "# val_dataset = Subset(dataset, val_indices)\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightgbm\n",
    "model = joblib.load(f'../데이터/Checkpoint/lightgbm/lightgbm_batch_64_ws_12.pkl')\n",
    "\n",
    "# catboost\n",
    "# model = joblib.load(f'../데이터/Checkpoint/catboost/catboost_batch_64_ws_12.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_pred, y_true):\n",
    "      mse = np.mean((y_true - y_pred) ** 2)\n",
    "      return np.sqrt(mse)\n",
    "\n",
    "def mse(y_pred, y_true):\n",
    "      return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mae(y_pred, y_true):\n",
    "      return np.mean(np.abs(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 20.7914\n",
      "Test MSE: 432.2820\n",
      "Test MAE: 20.7664\n"
     ]
    }
   ],
   "source": [
    "test_rmses = []\n",
    "test_mses = []\n",
    "test_maes = []\n",
    "\n",
    "test_outputs = []\n",
    "test_trgs = []\n",
    "\n",
    "for data in test_dataloader:\n",
    "    X, y = data[0].squeeze().cpu().numpy(), data[1].squeeze().cpu().numpy()\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    test_outputs.append(y_pred)\n",
    "    test_trgs.append(y)\n",
    "\n",
    "save_path = f'../데이터/Checkpoint/lightgbm/lightgbm_batch_{batch}_ws_{window_size}'\n",
    "with open(f'{save_path}_test_rmses.txt', 'w') as f:\n",
    "    for item in test_rmses:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "with open(f'{save_path}_test_mses.txt', 'w') as f:\n",
    "    for item in test_mses:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "with open(f'{save_path}_test_maes.txt', 'w') as f:\n",
    "    for item in test_maes:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "test_rmse = rmse(y_pred, y)\n",
    "test_mse = mse(y_pred, y)\n",
    "test_mae = mae(y_pred, y)\n",
    "\n",
    "print(f'Test RMSE: {test_rmse:.4f}')\n",
    "print(f'Test MSE: {test_mse:.4f}')\n",
    "print(f'Test MAE: {test_mae:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
